{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tabulate\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "# from torchtext import data\n",
    "# from torchtext import datasets\n",
    "# from torchtext.vocab import Vectors, GloVe\n",
    "\n",
    "import curves\n",
    "import data\n",
    "import load_data\n",
    "import models\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weight_ih_l0_5'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 5\n",
    "layer = 0\n",
    "suffix = ''\n",
    "'weight_ih_l{}{}_{}'.format(layer, suffix, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_dir = 'saved_models/' # training directory\n",
    "\n",
    "data_path = './data/' # path to datasets location \n",
    "# dataset = 'CIFAR10'   # dataset name\n",
    "# transform = 'VGG'     # transform name\n",
    "batch_size = 32       # input batch size\n",
    "num_workers = 4       # number of workers\n",
    "use_test = False      # switches between validation and test set (default: validation)\n",
    "model_name = 'LSTMClassifier'  # model name\n",
    "\n",
    "init_start = './saved_models/LSTMClassifier-6.pt'     # checkpoint to init start point. metavar='CKPT'\n",
    "init_end = './saved_models/LSTMClassifier2-4.pt'       # checkpoint to init end point. metavar='CKPT'\n",
    "\n",
    "fix_start = True      # fix start point\n",
    "fix_end = True        # fix end point\n",
    "\n",
    "wd = 1e-5             # weight decay\n",
    "Momentum = 0.9        # SGD momentum\n",
    "LR = 1e-3             # initial learning rate\n",
    "resume = None         # checkpoint to resume training from. metavar='CKPT'\n",
    "Epochs = 200          # number of epochs to train\n",
    "\n",
    "save_freq = 1        # save frequency\n",
    "num_bends = 3         # number of curve bends\n",
    "curve_type = 'PolyChain'     # Bezier/PolyChain\n",
    "reg = False\n",
    "init_linear = True    # linear initialization of intermediate points\n",
    "\n",
    "seed_val = 37          # random seed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Text Vocabulary: 135872\n",
      "Vector size of Text Vocabulary:  torch.Size([135872, 300])\n",
      "Label Length: 4\n"
     ]
    }
   ],
   "source": [
    "TEXT, vocab_size, num_classes, word_embeddings, train_loader, valid_loader, test_loader = \\\n",
    "                                load_data.load_dataset(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ./saved_models/LSTMClassifier-6.pt as point #0\n",
      "> \u001b[0;32m/home/hakobtamazyan/dnn-mode-connectivity/curves.py\u001b[0m(522)\u001b[0;36mimport_base_parameters\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m    520 \u001b[0;31m        \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    521 \u001b[0;31m        \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m--> 522 \u001b[0;31m        \u001b[0mbase_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    523 \u001b[0;31m        \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_parameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m    524 \u001b[0;31m            \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_parameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> len(self.net.parameters())\n",
      "*** TypeError: object of type 'generator' has no len()\n",
      "ipdb> len(list(self.net.parameters()))\n",
      "19\n",
      "ipdb> list(self.net._all_weights)\n",
      "*** Error in argument: '(self.net._all_weights)'\n",
      "ipdb> self.net\n",
      "LSTMClassifierCurve(\n",
      "  (word_embeddings): Embedding(135872, 300)\n",
      "  (lstm): LSTM(300, 256)\n",
      "  (label): Linear()\n",
      ")\n",
      "ipdb> list(self.net.lstm._all_weights)\n",
      "*** Error in argument: '(self.net.lstm._all_weights)'\n",
      "ipdb> self.net.lstm\n",
      "LSTM(300, 256)\n",
      "ipdb> self.net.lstm..input_size\n",
      "*** SyntaxError: invalid syntax\n",
      "ipdb> self.net.lstm.input_size\n",
      "300\n",
      "ipdb> self.net.lstm._all_weights\n",
      "[['weight_ih_l0_0', 'weight_ih_l0_1', 'weight_ih_l0_2', 'weight_hh_l0_0', 'weight_hh_l0_1', 'weight_hh_l0_2', 'bias_ih_l0_0', 'bias_ih_l0_1', 'bias_ih_l0_2', 'bias_hh_l0_0', 'bias_hh_l0_1', 'bias_hh_l0_2']]\n",
      "ipdb> len(self.net.lstm.parameters())\n",
      "*** TypeError: object of type 'generator' has no len()\n",
      "ipdb> len(len(self.net.lstm.parameters()))\n",
      "*** TypeError: object of type 'generator' has no len()\n",
      "ipdb> len(list(self.net.lstm.parameters()))\n",
      "12\n",
      "ipdb> len(list(self.net.lstm.Embedding()))\n",
      "*** AttributeError: 'LSTM' object has no attribute 'Embedding'\n",
      "ipdb> len(list(self.net. Embedding.parameters()))\n",
      "*** AttributeError: 'LSTMClassifierCurve' object has no attribute 'Embedding'\n",
      "ipdb> len(list(self.net.Embedding.parameters()))\n",
      "*** AttributeError: 'LSTMClassifierCurve' object has no attribute 'Embedding'\n",
      "ipdb> len(list(self.net.word_embeddings.parameters()))\n",
      "1\n",
      "ipdb> q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-66c0db836b41>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading %s as point #%d'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                 \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_base_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minit_linear\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Linear initialization.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dnn-mode-connectivity/curves.py\u001b[0m in \u001b[0;36mimport_base_parameters\u001b[0;34m(self, base_model, index)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mbase_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_parameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_parameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dnn-mode-connectivity/curves.py\u001b[0m in \u001b[0;36mimport_base_parameters\u001b[0;34m(self, base_model, index)\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebugger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_trace\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m         \u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m         \u001b[0mbase_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_parameter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_parameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.5/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(c_dir, exist_ok=True)\n",
    "#TEXT, vocab_size, num_classes, word_embeddings, train_loader, valid_loader, test_loader = load_data.load_dataset(batch_size=batch_size)\n",
    "\n",
    "#num_classes = 4\n",
    "#learning_rate = 2e-5\n",
    "kwargs = {\n",
    "    'batch_size': batch_size,\n",
    "    'hidden_size': 256,\n",
    "    'embedding_length': 300,\n",
    "    'vocab_size': vocab_size,\n",
    "    'weights': word_embeddings\n",
    "}\n",
    "\n",
    "\n",
    "architecture = getattr(models, model_name)\n",
    "\n",
    "torch.backends.cudnn.benchmark = True\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed(seed_val)\n",
    "\n",
    "if curve_type is None:\n",
    "    model = architecture.base(num_classes=num_classes, **kwargs)\n",
    "else:\n",
    "    curve = getattr(curves, curve_type)\n",
    "    model = curves.CurveNet(\n",
    "        num_classes,\n",
    "        curve,\n",
    "        architecture.curve,\n",
    "        num_bends,\n",
    "        fix_start,\n",
    "        fix_end,\n",
    "        architecture_kwargs=kwargs,\n",
    "    )\n",
    "    base_model = None\n",
    "    if resume is None:\n",
    "        for path, k in [(init_start, 0), (init_end, num_bends - 1)]:\n",
    "            if path is not None:\n",
    "                if base_model is None:\n",
    "                    base_model = architecture.base(num_classes=num_classes, **kwargs)\n",
    "                checkpoint = torch.load(path)\n",
    "                print('Loading %s as point #%d' % (path, k))\n",
    "                base_model.load_state_dict(checkpoint['model_state'])\n",
    "                model.import_base_parameters(base_model, k)\n",
    "        if init_linear:\n",
    "            print('Linear initialization.')\n",
    "            model.init_linear()\n",
    "model.cuda()\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "regularizer = None if curve_type is None else curves.l2_regularizer(wd)\n",
    "# optimizer = torch.optim.SGD(\n",
    "#     filter(lambda param: param.requires_grad, model.parameters()),\n",
    "#     lr=LR,\n",
    "#     momentum=Momentum,\n",
    "#     weight_decay=wd if curve_type is None else 0.0\n",
    "# )\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "start_epoch = 1\n",
    "if resume is not None:\n",
    "    print('Resume training from %s' % resume)\n",
    "    checkpoint = torch.load(resume)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "\n",
    "name = 'LSTMClassifier_curve'\n",
    "utils.save_checkpoint(\n",
    "    c_dir,\n",
    "    start_epoch - 1,\n",
    "    name +'_regularizer' if reg else name,\n",
    "    model_state=model.state_dict(),\n",
    "    optimizer_state=optimizer.state_dict()\n",
    ")\n",
    "\n",
    "for epoch in range(start_epoch, Epochs + 1):\n",
    "    time_ep = time.time()\n",
    "\n",
    "#     lr = utils.learning_rate_schedule(LR, epoch, Epochs)\n",
    "#     utils.adjust_learning_rate(optimizer, lr)\n",
    "    \n",
    "    \n",
    "    train_res = utils.train_model(train_loader, model, optimizer, loss_fn, epoch, batch_size, regularizer)\n",
    "    val_res = utils.eval_model(valid_loader, model, loss_fn, batch_size, regularizer)\n",
    "#     if curve_type is None:\n",
    "#         test_res = eval_model(test_loader, model, loss_fn, regularizer)\n",
    "\n",
    "    if epoch % save_freq == 0:\n",
    "        utils.save_checkpoint(\n",
    "            c_dir,\n",
    "            epoch,\n",
    "            name + '_regularizer' if reg else name,\n",
    "            model_state=model.state_dict(),\n",
    "            optimizer_state=optimizer.state_dict()\n",
    "        )\n",
    "\n",
    "    time_ep = time.time() - time_ep\n",
    "    print('Epoch: {:02}, Train Loss: {:.3f}, Train Acc: {:.2f}%, Val. Loss: {:.3f}, Val. Acc: {:.2f}%'\\\n",
    "         .format(epoch, train_res['loss'], train_res['acc'], val_res['nll'], val_res['acc']))\n",
    "\n",
    "\n",
    "if Epochs % save_freq != 0:\n",
    "    utils.save_checkpoint(\n",
    "        c_dir,\n",
    "        Epochs,\n",
    "        name + '_regularizer' if reg else name,\n",
    "        model_state=model.state_dict(),\n",
    "        optimizer_state=optimizer.state_dict()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('LSTMClassifier',)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name +'2'+'_regularizer' if reg else model_name,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8af88ad5c534f969834732d67ed4b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 02, Loss: 0.270, Acc: 89.94%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0fda1be5fb14655991d82f48b524ba9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 03, Loss: 0.252, Acc: 90.93%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9630e729024fcb899679c0a91c1fd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 04, Loss: 0.246, Acc: 91.09%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af7d725554964fce91461c253b8701ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.251, Acc: 91.28%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ab1366e559d4a04807c858bc264599e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=563), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 06, Loss: 0.281, Acc: 90.71%\n"
     ]
    }
   ],
   "source": [
    "for epoch in np.arange(2,7):\n",
    "    name = 'LSTMClassifier2'\n",
    "    checkpoint = torch.load(os.path.join(c_dir, '%s-%d.pt' % (name, epoch)))\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state'])\n",
    "\n",
    "    res = eval_model(valid_loader, model, loss_fn, regularizer)\n",
    "    print('Epoch: {:02}, Loss: {:.3f}, Acc: {:.2f}%'.format(epoch, res['nll'], res['acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT, vocab_size, num_classes, word_embeddings, train_loader, valid_loader, test_loader = load_data.load_dataset()\n",
    "# for i, x in enumerate(train_loader):\n",
    "#     if i in range(3,7):\n",
    "#         break\n",
    "#     print(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT, vocab_size, num_classes, word_embeddings, train_loader, valid_loader, test_loader = load_data.load_dataset_imdb()\n",
    "# for i, x in enumerate(train_loader):\n",
    "#     if i in range(3,7):\n",
    "#         break\n",
    "#     print(x.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-32d8d9396dde>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1440\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m     \"\"\"\n\u001b[0;32m-> 1442\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/venv3/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel)\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    943\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'log_softmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 944\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "def train_model(train_loader, model, optimizer, loss_fn, epoch, regularizer=None):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_acc = 0\n",
    "    model.cuda()\n",
    "    #optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n",
    "    \n",
    "    steps = 0\n",
    "    model.train()\n",
    "    pbar = tqdm(enumerate(train_loader), total=len(train_loader))\n",
    "    for i, batch in pbar:\n",
    "        text = batch.text[0]\n",
    "        target = batch.label\n",
    "        target = torch.autograd.Variable(target).long()\n",
    "        if torch.cuda.is_available():\n",
    "            text = text.cuda()\n",
    "            target = target.cuda()\n",
    "        if (text.size()[0] is not batch_size):# One of the batch returned by BucketIterator has length different than batch_size.\n",
    "            continue\n",
    "            \n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(text)\n",
    "        loss = loss_fn(prediction, target)\n",
    "        if regularizer is not None:\n",
    "            loss += regularizer(model)\n",
    "\n",
    "        num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).float().sum()\n",
    "        acc = 100.0 * num_corrects/len(batch)\n",
    "        \n",
    "        loss.backward()\n",
    "        utils.clip_gradient(model, 1e-2)\n",
    "        optimizer.step()\n",
    "        steps += 1\n",
    "        \n",
    "#         if steps % 100 == 0:\n",
    "#             print ('Epoch: {}, Iter: {}, Training Loss: {:.4f}, Training acc: {:.2f}%'\\\n",
    "#                    .format(epoch, i+1, loss.item(), acc.item()))\n",
    "        \n",
    "        total_epoch_loss += loss.item()\n",
    "        total_epoch_acc += acc.item()\n",
    "        pbar.set_description_str('[TRAIN] Epoch: {}, Train Loss: {:.4f}, Train acc: {:.2f}%'\\\n",
    "            .format(epoch,\n",
    "                   total_epoch_loss / (i + 1),\n",
    "                   total_epoch_acc / (i + 1)))\n",
    "    \n",
    "    return {\n",
    "        'loss': total_epoch_loss/len(train_loader),\n",
    "        'acc': total_epoch_acc/len(train_loader)\n",
    "    }\n",
    "\n",
    "def eval_model(val_loader, model, loss_fn, batchregularizer=None):\n",
    "    total_epoch_loss = 0\n",
    "    total_epoch_nll = 0\n",
    "    total_epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, batch in tqdm(enumerate(val_loader), total=len(val_loader)):\n",
    "            text = batch.text[0]\n",
    "            if (text.size()[0] is not batch_size):\n",
    "                continue\n",
    "                \n",
    "            target = batch.label\n",
    "            target = torch.autograd.Variable(target).long()\n",
    "            if torch.cuda.is_available():\n",
    "                text = text.cuda()\n",
    "                target = target.cuda()\n",
    "            prediction = model(text)\n",
    "            \n",
    "            nll = loss_fn(prediction, target)\n",
    "            loss = nll.clone()\n",
    "            if regularizer is not None:\n",
    "                loss += regularizer(model)\n",
    "                \n",
    "            num_corrects = (torch.max(prediction, 1)[1].view(target.size()).data == target.data).sum()\n",
    "            acc = 100.0 * num_corrects/len(batch)\n",
    "            total_epoch_loss += loss.item()\n",
    "            total_epoch_nll += nll.item()\n",
    "            total_epoch_acc += acc.item()\n",
    "\n",
    "    return {\n",
    "        'nll': total_epoch_nll/len(val_loader),\n",
    "        'loss': total_epoch_loss/len(val_loader),\n",
    "        'acc': total_epoch_acc/len(val_loader)\n",
    "    }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
